import gradio as gr
import torch
from PIL import Image
from unsloth import FastVisionModel
from transformers import TextStreamer
import os
from utils.helpers import Logger

class SimpleMedicalUI:
    """
    ç®€å•çš„åŒ»å­¦å½±åƒå¯¹è¯UIç•Œé¢
    """
    
    def __init__(self, model_path):
        """
        åˆå§‹åŒ–UIç•Œé¢
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
        """
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
        
        Logger.info("åŒ»å­¦å½±åƒå¯¹è¯UIåˆå§‹åŒ–å®Œæˆ")
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        Logger.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")

        os.environ["TRANSFORMERS_OFFLINE"] = "1"
        os.environ["HF_HUB_OFFLINE"] = "1"
        os.environ["UNSLOTH_OFFLINE"] = "1"
        os.environ["HF_DATASETS_OFFLINE"] = "1"
        os.environ["UNSLOTH_DISABLE_TELEMETRY"] = "1"

        # ç¦ç”¨ç½‘ç»œè¿æ¥
        os.environ["NO_PROXY"] = "*"
        os.environ["http_proxy"] = ""
        os.environ["https_proxy"] = ""
        
        print("ğŸ”Œ å¼ºåˆ¶ç¦»çº¿æ¨¡å¼å·²å¯ç”¨")
        
        try:
            self.model, self.tokenizer = FastVisionModel.from_pretrained(
                model_name=self.model_path,
                load_in_4bit=True,
                local_files_only=True
            )
            FastVisionModel.for_inference(self.model)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def generate_response(self, image, question, max_new_tokens=256, temperature=1.5, min_p=0.2):
        """
        ç”Ÿæˆæ¨¡å‹å“åº” - ä½¿ç”¨æ‚¨æµ‹è¯•ç¨‹åºä¸­çš„æ–¹æ³•
        
        Args:
            image: ä¸Šä¼ çš„å›¾åƒ
            question: ç”¨æˆ·é—®é¢˜
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: ç”Ÿæˆæ¸©åº¦
            min_p: æœ€å°æ¦‚ç‡é˜ˆå€¼
            
        Returns:
            response: æ¨¡å‹å“åº”
        """
        if image is None:
            return "è¯·å…ˆä¸Šä¼ ä¸€å¼ åŒ»å­¦å½±åƒå›¾ç‰‡ã€‚"
        
        try:
            # ç›´æ¥ä½¿ç”¨Gradioè¿”å›çš„å›¾åƒï¼Œå®ƒå·²ç»æ˜¯PILæ ¼å¼
            # Gradioçš„Imageç»„ä»¶(type="pil")ä¼šç›´æ¥è¿”å›PIL.Imageå¯¹è±¡
            pil_image = image
            
            # ç¡®ä¿å›¾åƒæ˜¯RGBæ¨¡å¼
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
            
            # ä½¿ç”¨æ‚¨æµ‹è¯•ç¨‹åºä¸­çš„æŒ‡ä»¤
            if not question or question.strip() == "":
                instruction = "ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ”¾å°„ç§‘åŒ»ç”Ÿã€‚è¯·å‡†ç¡®æè¿°ä½ åœ¨å›¾ç‰‡ä¸­çœ‹åˆ°çš„å†…å®¹ã€‚"
            else:
                instruction = question
            
            # ä½¿ç”¨æ‚¨æµ‹è¯•ç¨‹åºä¸­çš„æ¶ˆæ¯æ ¼å¼
            messages = [
                {
                    "role": "user", 
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": instruction}
                    ]
                }
            ]
            
            # ä½¿ç”¨æ‚¨æµ‹è¯•ç¨‹åºä¸­çš„æ–¹æ³•
            input_text = self.tokenizer.apply_chat_template(
                messages, 
                add_generation_prompt=True
            )
            
            # ç›´æ¥ä¼ é€’PILå›¾åƒç»™tokenizer
            inputs = self.tokenizer(
                pil_image,
                input_text,
                add_special_tokens=False,  # å¦‚æ‚¨æµ‹è¯•ç¨‹åºä¸­æ‰€è¿°
                return_tensors='pt',
            ).to(self.device)
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    use_cache=True,
                    temperature=temperature,
                    min_p=min_p,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç å“åº”
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–ç”Ÿæˆçš„å“åº”éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥ï¼‰
            if input_text in response:
                response = response.replace(input_text, "").strip()
            
            # æ¸…ç†å“åº”æ–‡æœ¬
            response = self.clean_response(response)
            
            return response
            
        except Exception as e:
            error_msg = f"ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"
            Logger.error(error_msg)
            import traceback
            traceback.print_exc()
            return error_msg
    
    def clean_response(self, response):
        """
        æ¸…ç†æ¨¡å‹å“åº”ï¼Œç§»é™¤ä¸å¿…è¦çš„æ ‡è®°å’Œé‡å¤å†…å®¹
        
        Args:
            response: åŸå§‹æ¨¡å‹å“åº”
            
        Returns:
            cleaned_response: æ¸…ç†åçš„å“åº”
        """
        # ç§»é™¤ç‰¹æ®Šæ ‡è®°
        special_tokens = ["<|endoftext|>", "<|im_end|>", "<|im_start|>"]
        for token in special_tokens:
            response = response.replace(token, "")
        
        return response.strip()
    
    def test_model(self):
        """
        æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œ
        """
        try:
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ
            test_image = Image.new('RGB', (256, 256), color=(100, 100, 100))
            
            # æµ‹è¯•é—®é¢˜
            test_question = "æè¿°è¿™å¼ å›¾ç‰‡"
            
            # ç”Ÿæˆå“åº”
            response = self.generate_response(test_image, test_question)
            
            if response and "å‡ºé”™" not in response:
                print("âœ… æ¨¡å‹æµ‹è¯•æˆåŠŸ!")
                print(f"æµ‹è¯•å“åº”: {response}")
                return True
            else:
                print("âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥!")
                return False
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹æµ‹è¯•å‡ºé”™: {e}")
            return False
    
    def create_interface(self):
        """åˆ›å»ºGradioç•Œé¢"""
        
        # å®šä¹‰CSSæ ·å¼
        css = """
        .gradio-container {
            max-width: 1000px !important;
        }
        .medical-title {
            text-align: center;
            color: #2c3e50;
            margin-bottom: 20px;
            font-family: Arial, sans-serif;
        }
        """
        
        # ç¤ºä¾‹é—®é¢˜åˆ—è¡¨
        example_questions = [
            "ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ”¾å°„ç§‘åŒ»ç”Ÿã€‚è¯·å‡†ç¡®æè¿°ä½ åœ¨å›¾ç‰‡ä¸­çœ‹åˆ°çš„å†…å®¹ã€‚",
            "è¯·æè¿°è¿™å¼ åŒ»å­¦å½±åƒä¸­çš„å¼‚å¸¸å‘ç°ã€‚",
            "æ ¹æ®è¿™å¼ å½±åƒï¼Œä½ çš„è¯Šæ–­æ„è§æ˜¯ä»€ä¹ˆï¼Ÿ",
            "è¯·è¯¦ç»†æè¿°å½±åƒä¸­çš„è§£å‰–ç»“æ„å’Œå¯èƒ½çš„ç—…ç†å˜åŒ–ã€‚",
            "è¿™å¼ å½±åƒæ˜¯å¦æ˜¾ç¤ºä»»ä½•å¼‚å¸¸ï¼Ÿå¦‚æœæœ‰ï¼Œè¯·æè¿°ã€‚",
            "å½±åƒä¸­çš„è¿™äº›ç‰¹å¾å¯èƒ½è¡¨ç¤ºä»€ä¹ˆç–¾ç—…ï¼Ÿ"
        ]
        
        with gr.Blocks(css=css, title="åŒ»å­¦å½±åƒAIåŠ©æ‰‹") as interface:
            gr.Markdown(
                """
                # ğŸ¥ åŒ»å­¦å½±åƒAIåŠ©æ‰‹
                **ä¸Šä¼ åŒ»å­¦å½±åƒå›¾ç‰‡ï¼Œä¸ä¸“ä¸šçš„AIæ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œå¯¹è¯**
                """,
                elem_classes="medical-title"
            )
            
            with gr.Row():
                with gr.Column(scale=1):
                    # å›¾åƒä¸Šä¼ åŒºåŸŸ - ä½¿ç”¨pilç±»å‹ï¼Œç›´æ¥è¿”å›PILå›¾åƒ
                    image_input = gr.Image(
                        label="ä¸Šä¼ åŒ»å­¦å½±åƒ",
                        type="pil",  # ç›´æ¥è¿”å›PILå›¾åƒå¯¹è±¡
                        height=300
                    )
                    
                    # ç¤ºä¾‹é—®é¢˜
                    gr.Markdown("### ğŸ’¡ ç¤ºä¾‹é—®é¢˜")
                    
                    # å­˜å‚¨ç¤ºä¾‹æŒ‰é’®çš„åˆ—è¡¨
                    example_buttons = []
                    
                    # åˆ›å»ºç¤ºä¾‹é—®é¢˜æŒ‰é’®ä½†ä¸ç«‹å³ç»‘å®šäº‹ä»¶
                    for question in example_questions:
                        btn = gr.Button(
                            question, 
                            size="sm"
                        )
                        example_buttons.append((btn, question))
                    
                    # å‚æ•°è°ƒæ•´
                    with gr.Accordion("âš™ï¸ é«˜çº§è®¾ç½®", open=False):
                        max_tokens = gr.Slider(
                            minimum=64,
                            maximum=512,
                            value=256,
                            step=32,
                            label="æœ€å¤§ç”Ÿæˆé•¿åº¦"
                        )
                        temperature = gr.Slider(
                            minimum=0.1,
                            maximum=1.0,
                            value=0.5,
                            step=0.1,
                            label="ç”Ÿæˆæ¸©åº¦"
                        )
                        min_p = gr.Slider(
                            minimum=0.0,
                            maximum=0.5,
                            value=0.2,
                            step=0.05,
                            label="æœ€å°æ¦‚ç‡é˜ˆå€¼"
                        )
                
                with gr.Column(scale=2):
                    # é—®é¢˜è¾“å…¥
                    question_input = gr.Textbox(
                        label="è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="è¯·è¾“å…¥å…³äºè¿™å¼ åŒ»å­¦å½±åƒçš„é—®é¢˜...",
                        lines=3
                    )
                    
                    # æäº¤æŒ‰é’®
                    submit_btn = gr.Button("åˆ†æå½±åƒ", variant="primary", size="lg")
                    
                    # ç»“æœæ˜¾ç¤º
                    output = gr.Textbox(
                        label="AIåˆ†æç»“æœ",
                        interactive=False,
                        lines=8
                    )
            
            # ä¸ºç¤ºä¾‹æŒ‰é’®ç»‘å®šäº‹ä»¶ï¼ˆåœ¨æ‰€æœ‰ç»„ä»¶å®šä¹‰å®Œæˆåï¼‰
            for btn, question in example_buttons:
                def make_click_handler(q):
                    def handler():
                        return q
                    return handler
                
                btn.click(
                    make_click_handler(question),
                    inputs=None,
                    outputs=question_input
                )
            
            # æäº¤é—®é¢˜
            submit_btn.click(
                fn=self.generate_response,
                inputs=[
                    image_input, 
                    question_input, 
                    max_tokens, 
                    temperature, 
                    min_p
                ],
                outputs=output
            )
            
            # ç¤ºä¾‹è¯´æ˜
            gr.Markdown(
                """
                ### ä½¿ç”¨è¯´æ˜:
                1. **ä¸Šä¼ å›¾ç‰‡**: ç‚¹å‡»ä¸Šä¼ æŒ‰é’®é€‰æ‹©åŒ»å­¦å½±åƒå›¾ç‰‡ï¼ˆæ”¯æŒJPGã€PNGæ ¼å¼ï¼‰
                2. **è¾“å…¥é—®é¢˜**: åœ¨è¾“å…¥æ¡†è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ–ç‚¹å‡»ç¤ºä¾‹é—®é¢˜
                3. **è·å–åˆ†æ**: ç‚¹å‡»"åˆ†æå½±åƒ"æŒ‰é’®ï¼ŒAIæ”¾å°„ç§‘åŒ»ç”Ÿå°†åˆ†æå½±åƒå¹¶æä¾›ä¸“ä¸šæè¿°
                
                ### æ³¨æ„äº‹é¡¹:
                - æœ¬ç³»ç»Ÿä»…ä¾›åŒ»å­¦ç ”ç©¶å’Œæ•™å­¦ä½¿ç”¨ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­
                - ä¸Šä¼ çš„å›¾ç‰‡ä»…ç”¨äºæœ¬æ¬¡åˆ†æï¼Œä¸ä¼šå­˜å‚¨
                - å¦‚é‡ç´§æ€¥åŒ»ç–—æƒ…å†µï¼Œè¯·ç«‹å³è”ç³»ä¸“ä¸šåŒ»ç–—æœºæ„
                """
            )
        
        return interface

def main():
    """å¯åŠ¨åŒ»å­¦å½±åƒå¯¹è¯UI"""
    
    # æ¨¡å‹è·¯å¾„
    model_path = "./lora_model"  #å¾®è°ƒåçš„æ¨¡å‹
    # model_path = "./models/unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit"
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    # åˆ›å»ºUIå®ä¾‹
    try:
        medical_ui = SimpleMedicalUI(model_path)
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•æ¨¡å‹
    # print("ğŸ§ª æµ‹è¯•æ¨¡å‹...")
    # test_result = medical_ui.test_model()
    
    # if not test_result:
    #     print("âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹")
    #     return
    
    # åˆ›å»ºç•Œé¢
    interface = medical_ui.create_interface()
    
    # å¯åŠ¨æœåŠ¡
    print("ğŸš€ å¯åŠ¨åŒ»å­¦å½±åƒAIåŠ©æ‰‹...")
    print("ğŸ“± è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:6006")
    print("â¹ï¸ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    
    interface.launch(
        server_name="0.0.0.0",
        server_port=6008,
        share=False,
    )

if __name__ == "__main__":
    main()